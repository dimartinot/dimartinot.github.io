<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>ipynb | DIMARTINOT</title>
    <link>https://dimartinot.github.io/tags/ipynb/</link>
      <atom:link href="https://dimartinot.github.io/tags/ipynb/index.xml" rel="self" type="application/rss+xml" />
    <description>ipynb</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Mon, 01 Jun 2020 20:01:00 +0000</lastBuildDate>
    <image>
      <url>https://dimartinot.github.io/img/shanghai_wetland.png</url>
      <title>ipynb</title>
      <link>https://dimartinot.github.io/tags/ipynb/</link>
    </image>
    
    <item>
      <title>Deep Similarity Learning &amp; Siamese Networks</title>
      <link>https://dimartinot.github.io/project/deep_similarity_learning/</link>
      <pubDate>Mon, 01 Jun 2020 20:01:00 +0000</pubDate>
      <guid>https://dimartinot.github.io/project/deep_similarity_learning/</guid>
      <description>&lt;p&gt;In this project, I explored deep similarity learning algorithms and their behaviour with different types of data (sequential data, spatial data, multimodal data). For each of these different modalities, I wrote 2 Medium article detailing the retained method and providing my implementation.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;First blog post: &lt;a href=&#34;https://medium.com/@dimartinot/introduction-to-deep-similarity-learning-for-sequences-89d9c26f8392&#34;&gt;Introduction to Deep Similarity learning for sequences&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Second blog post: &lt;a href=&#34;https://towardsdatascience.com/how-to-choose-your-loss-when-designing-a-siamese-neural-net-contrastive-triplet-or-quadruplet-ecba11944ec?source=friends_link&amp;sk=8e0ca4642ae140db03fc83ecf60daf9d&#34;&gt;How to choose your loss when designing a Siamese Neural Network ? Contrastive, Triplet or Quadruplet ?&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Deep Reinforcement Learning projects</title>
      <link>https://dimartinot.github.io/project/deep_rl_projects/</link>
      <pubDate>Thu, 21 May 2020 20:00:00 +0000</pubDate>
      <guid>https://dimartinot.github.io/project/deep_rl_projects/</guid>
      <description>&lt;p&gt;These 3 projects are implementations made for the udacity&amp;rsquo;s nanodegree program, all passed through a reviewer. They contain a small report, gathering my comprehension fo the algorithm as well as details on my implementation and my parameters.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;First project: &lt;a href=&#34;https://github.com/dimartinot/P1-Navigation&#34;&gt;P1 Navigation&lt;/a&gt;. &lt;br&gt;
  Implementation of a DQN algorithm with uniformly sampled as well as prioritized Replay Buffer, with learning performance comparison.
  &lt;iframe src=&#34;https://www.linkedin.com/embed/feed/update/urn:li:ugcPost:6665990962071896064?compact=1&#34; allowfullscreen=&#34;&#34; title=&#34;Post intégré&#34; width=&#34;504&#34; height=&#34;284&#34; frameborder=&#34;0&#34;&gt;&lt;/iframe&gt;
  &lt;/li&gt;
  &lt;li&gt;Second project: &lt;a href=&#34;https://github.com/dimartinot/P2-Continuous-Control&#34;&gt;P2 Continuous Control&lt;/a&gt;. &lt;br&gt;
  Implementation of a DDPG algorithm with uniformly sampled Replay Buffer and UONoise modeling exploration. Soft update was also used between target and local networks.
  &lt;img width=&#34;504&#34; height=&#34;284&#34; src=&#34;env_screen_2.png&#34;&gt;
  &lt;/li&gt;
  &lt;li&gt;Third project: &lt;a href=&#34;https://github.com/dimartinot/P3-Collaborative&#34;&gt;P3 Collaborative Navigation&lt;/a&gt;. &lt;br&gt;
  Implementation of a DQN algorithm with uniformly sampled as well as prioritized Replay Buffer, with learning performance comparison.
  &lt;iframe src=&#34;https://www.linkedin.com/embed/feed/update/urn:li:ugcPost:6670628364702781440?compact=1&#34; allowfullscreen=&#34;&#34; title=&#34;Post intégré&#34; width=&#34;504&#34; height=&#34;284&#34; frameborder=&#34;0&#34;&gt;&lt;/iframe&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Time Series Land Cover Challenge: a Deep Learning Perspective</title>
      <link>https://dimartinot.github.io/project/tiselac/</link>
      <pubDate>Mon, 24 Feb 2020 20:09:00 +0000</pubDate>
      <guid>https://dimartinot.github.io/project/tiselac/</guid>
      <description>&lt;p&gt;In this project, I explored a Time Series of satellite images dataset by building different deep learning classifiers, finding inspiration in paper research in the field of Time Series classification.
Our dataset comprises of 23 images where each pixel is 10 Dimensional.&lt;/p&gt;
&lt;p&gt;I firstly took two different perspective when working with the dataset:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Having 10 unimodal models that I would then concatenate into a single model to retrieve the class of each pixel (I was inspired by the Time-CNN model). I wanted to exploit a presumed independence between the different features of the mutlimodal time series.&lt;/li&gt;
&lt;li&gt;Having one multimodal model that would work on the 10 Time Series of each pixel at once. There, I focused on presumed correlation between Time Series&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Finally, I designed a bigger model that would comprises of these two ideas to extract features from both a unimodal point of view and a multimodal point of view.&lt;/p&gt;
&lt;p&gt;My final model architecture is the following:&lt;/p&gt;
&lt;figure&gt;
  &lt;img src=&#34;final_architecture.png&#34; style=&#34;background-color:lightsteelblue&#34;&gt;
  &lt;figcaption&gt;Architecture of my proposed final network, combination of 3 different models&lt;/figcaption&gt;
&lt;/figure&gt;</description>
    </item>
    
    <item>
      <title>Segmentation Models on artificial moon imagery</title>
      <link>https://dimartinot.github.io/project/artificial_moon/</link>
      <pubDate>Sun, 20 Oct 2019 00:00:00 +0000</pubDate>
      <guid>https://dimartinot.github.io/project/artificial_moon/</guid>
      <description>&lt;p&gt;In this project, I trained 4 deep learning segmentation models on an artificial Lunar Dataset to see how they will perform on real images from Nasa.
For this project, I trained and tested 4 different segmentation models:&lt;/p&gt;
&lt;ul&gt;
   &lt;li&gt; UNet
   &lt;li&gt; LinkNet
   &lt;li&gt; PSPNet
   &lt;li&gt; FPN
&lt;/ul&gt;
 All of them had very similar training procedure, you can therefore consult the notebook I used to train the FPN and extrapolate the main components of it to the others.
 The second notebook is where I tested my model on the test dataset and on real moon images. This dataset comes from &lt;a href=&#34;https://www.kaggle.com/romainpessia/artificial-lunar-rocky-landscape-dataset&#34;&gt;kaggle&lt;/a&gt;.
 I worked with:
 &lt;ul&gt;
  &lt;li&gt; Around 7000 images for train set
  &lt;li&gt; Around 2000 images for validation set
  &lt;li&gt; Around 1000 images for the test set
  &lt;li&gt; Around 40 images from real moon pictures
&lt;/ul&gt;
I then tried my model on an Apollo video shot from a rover driven during the 1972&#39;s Apollo 15 mission. All these results are consultable in my presentation video.
&lt;iframe width=&#34;720&#34; height=&#34;480&#34;
  allowfullscreen=&#34;allowfullscreen&#34;
        mozallowfullscreen=&#34;mozallowfullscreen&#34;
        msallowfullscreen=&#34;msallowfullscreen&#34;
        oallowfullscreen=&#34;oallowfullscreen&#34;
        webkitallowfullscreen=&#34;webkitallowfullscreen&#34;
src=&#34;https://www.youtube.com/embed/pw6Jz4lX2Kc&#34;&gt;
&lt;/iframe&gt;
&lt;p&gt;You can also dive into my code in these multiple notebooks !&lt;/p&gt;
&lt;div class=&#34;row&#34;&gt;
  &lt;div class=&#34;col&#34;&gt;
    &lt;button class=&#34;btn btn-primary&#34; onclick=&#34;window.open(&#39;/notebooks/main_presentation_notebook&#39;)&#34;&gt; Open test notebook &lt;/button&gt;
  &lt;/div&gt;
  &lt;div class=&#34;col&#34;&gt;
    &lt;button class=&#34;btn btn-primary&#34; onclick=&#34;window.open(&#39;/notebooks/segmentation_moon_dataset_fpn&#39;)&#34;&gt; Open FPN training notebook &lt;/button&gt;
  &lt;/div&gt;
  &lt;br&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
